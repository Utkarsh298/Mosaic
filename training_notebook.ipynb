{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f4cf4804",
   "metadata": {},
   "source": [
    "### Importing necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0fca706",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import librosa\n",
    "import librosa.display\n",
    "import skimage.io\n",
    "import os\n",
    "import cv2\n",
    "from pydub import AudioSegment\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras.layers import Dense, Conv2D, MaxPool2D, MaxPooling2D, BatchNormalization, Flatten, Dropout, ZeroPadding2D, concatenate, Input\n",
    "from keras.models import Model\n",
    "from keras import regularizers\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2b106e21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to run matplotlib in jupyter properly\n",
    "os.environ['KMP_DUPLICATE_LIB_OK'] = 'True'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "80ef0e1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# path to datasest\n",
    "trainDB = \"Mosaic23_PS1_TrainData/ICBHI_final_database\"\n",
    "\n",
    "# list of all files\n",
    "train_files = os.listdir(trainDB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6ad09b5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# seperating txt and audio files into seperate lists\n",
    "\n",
    "wav_files = []\n",
    "txt_files = []\n",
    "\n",
    "for trfile in train_files:\n",
    "    if trfile[-3:] == \"txt\":\n",
    "        txt_files.append(trfile)\n",
    "    elif trfile[-3:] == \"wav\":\n",
    "        wav_files.append(trfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7e956cd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting breathing cycle and wheeze, crackle data from the text files\n",
    "\n",
    "breathing_cycles = {}\n",
    "crackle_wheeze = {}\n",
    "\n",
    "for txfile in txt_files:\n",
    "    txpath = os.path.join(trainDB, txfile)\n",
    "    t = open(txpath, 'r')\n",
    "    content = t.readlines()\n",
    "    breathing_cycles[txfile[:-4]] = []\n",
    "    crackle_wheeze[txfile[:-4]] = []\n",
    "    \n",
    "    for line in content:\n",
    "        data = line[:-1].split(\"\\t\")\n",
    "        breathing_cycles[txfile[:-4]].append((data[0], data[1]))\n",
    "        crackle_wheeze[txfile[:-4]].append((data[2], data[3]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76acd9c0",
   "metadata": {},
   "source": [
    "### Dividing audio files into seperate on basis of timestamps for breathing cycle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41679dc7",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "os.mkdir(\"parted_audfiles\")\n",
    "\n",
    "for wfile in wav_files:\n",
    "    wf_path = os.path.join(trainDB, wfile)\n",
    "    bcs = breathing_cycles[wfile[:-4]]\n",
    "    cws = crackle_wheeze[wfile[:-4]]\n",
    "    i = 0\n",
    "    \n",
    "    for bc in bcs:\n",
    "        i += 1\n",
    "        start = float(bc[0]) * 1000\n",
    "        end = float(bc[1]) * 1000\n",
    "        \n",
    "        AS = AudioSegment.from_wav(wf_path)\n",
    "        AS = AS[start: end]\n",
    "        AS.export(\"parted_audfiles/\" + wfile[:-4] + \"{}\".format(i) + \".wav\", format = \"wav\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "efbed82f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting list of parted audiofile names from directory\n",
    "\n",
    "part_wav_files = os.listdir(\"parted_audfiles\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4223ad86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Storing extra features obtained from file name to dictionary\n",
    "\n",
    "extra_feats = {}\n",
    "cls = {'Ar': 0, 'Tc': 1, 'Al': 2, 'Pl': 3, 'Pr': 4, 'Ll': 5, 'Lr': 6}\n",
    "re = {'LittC2SE': 0, 'Meditron': 1, 'Litt3200': 2, 'AKGC417L': 3}\n",
    "\n",
    "for pwfile in part_wav_files:\n",
    "    pwpath = os.path.join(trainDB, pwfile)\n",
    "    extra_feats[pwfile] = []\n",
    "    \n",
    "    extra_feats[pwfile].extend([cls[pwfile[8: 10]], re[pwfile[14: 22]]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e3b6b666",
   "metadata": {},
   "outputs": [],
   "source": [
    "# converting the features to list\n",
    "\n",
    "hard_feats = np.array(list(extra_feats.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92921514",
   "metadata": {},
   "outputs": [],
   "source": [
    "hard_feats"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af21c106",
   "metadata": {},
   "source": [
    "### Getting spectrograms from parted audiofiles to train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfcceefd",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "os.mkdir(\"parted_spectrograms\")\n",
    "\n",
    "def scale_minmax(X, min=0.0, max=1.0):\n",
    "    X_std = (X - X.min()) / (X.max() - X.min())\n",
    "    X_scaled = X_std * (max - min) + min\n",
    "    return X_scaled\n",
    "\n",
    "spectrograms = []\n",
    "srs = []\n",
    "\n",
    "for pwfile in part_wav_files:\n",
    "    pwf_path = os.path.join(\"parted_audfiles\", pwfile)\n",
    "    aud, sr = librosa.load(pwf_path)\n",
    "    \n",
    "    S = librosa.feature.melspectrogram(y=aud, sr=sr, n_mels=128, fmax=8000, hop_length=512)\n",
    "    \n",
    "    fig, ax = plt.subplots()\n",
    "    S_dB = librosa.power_to_db(S, ref=np.max)\n",
    "    S_dB = scale_minmax(S_dB, 0, 255).astype(np.uint8)\n",
    "    S_dB = np.flip(S_dB, axis = 0)\n",
    "    S_dB = 255 - S_dB\n",
    "    S_dB = cv2.resize(S_dB, (128, 128), interpolation = cv2.INTER_CUBIC)\n",
    "\n",
    "    skimage.io.imsave(\"parted_spectrograms/\" + pwfile[:-4] + \"spec.png\", S_dB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a20b8100",
   "metadata": {},
   "outputs": [],
   "source": [
    "# list of parted spectrograms from directory\n",
    "pspecs = os.listdir(\"parted_spectrograms\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3e8ba55",
   "metadata": {},
   "source": [
    "### Creating dataset, x -> spectrogram images, y -> labels in form [crackle_presence, wheeze_presence]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9ea3ec03",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_specs = []\n",
    "Y_labels = []\n",
    "\n",
    "for pspec in pspecs:\n",
    "    pspec_img = cv2.imread(os.path.join(\"parted_spectrograms\", pspec))\n",
    "    X_specs.append(pspec_img)\n",
    "\n",
    "for sfile in wav_files:\n",
    "    cws = crackle_wheeze[sfile[:-4]]\n",
    "    for cw in cws:\n",
    "        Y_labels.append(np.array(cw).astype(\"float32\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dd3ba7dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalizing the data\n",
    "for i in range(len(X_specs)):\n",
    "    X_specs[i] = X_specs[i] / 255.0\n",
    "    X_specs[i] = X_specs[i].reshape(X_specs[i].shape[0:3])\n",
    "    X_specs[i] = np.array(X_specs[i].astype(\"float32\"))\n",
    "    \n",
    "    Y_labels[i] = Y_labels[i].reshape(Y_labels[i].shape[0: 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b1e808eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(128, 128, 3)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_specs[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bd812d97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dividing into train and test datasets\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(list(zip(X_specs, hard_feats)), Y_labels, test_size = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "439e2175",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dividing further to create test and validation dataset\n",
    "\n",
    "X_test, X_val, Y_test, Y_val = train_test_split(X_test, Y_test, test_size = 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2585eeb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dividing train x set into spectrogram and extra features from file names\n",
    "\n",
    "train_hard = []\n",
    "train_x = []\n",
    "test_hard = []\n",
    "test_x = []\n",
    "val_hard = []\n",
    "val_x = []\n",
    "\n",
    "for e in X_train:\n",
    "    train_hard.append(e[1])\n",
    "    train_x.append(e[0])\n",
    "\n",
    "for e in X_test:\n",
    "    test_hard.append(e[1])\n",
    "    test_x.append(e[0])\n",
    "    \n",
    "for e in X_val:\n",
    "    val_hard.append(e[1])\n",
    "    val_x.append(e[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "22b353e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# converting all into numpy array\n",
    "\n",
    "train_hard = np.array(train_hard)\n",
    "train_x = np.array(train_x)\n",
    "test_hard = np.array(test_hard)\n",
    "test_x = np.array(test_x)\n",
    "val_hard = np.array(val_hard)\n",
    "val_x = np.array(val_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5ae0a6c",
   "metadata": {},
   "source": [
    "### Creating CNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "668f8074",
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_block():\n",
    "    return keras.Sequential([\n",
    "        ZeroPadding2D(padding = (1, 1)),\n",
    "        Conv2D(64, 3, padding = \"valid\", activation = \"ReLU\"),\n",
    "        BatchNormalization(axis = -1),\n",
    "        MaxPool2D(pool_size = 2)\n",
    "    ])\n",
    "\n",
    "inputs = Input(shape = (128, 128, 3))\n",
    "hard_feats = Input(shape = (1, ))\n",
    "x = conv_block()(inputs)\n",
    "x = conv_block()(x)\n",
    "x = conv_block()(x)\n",
    "x = conv_block()(x)\n",
    "x = Flatten()(x)\n",
    "x = Dense(32)(x)\n",
    "x = concatenate([x, hard_feats])\n",
    "x = BatchNormalization(axis = -1)(x)\n",
    "x = Dense(2, activation = \"sigmoid\")(x)\n",
    "\n",
    "model = Model(inputs = [inputs, hard_feats], outputs = [x])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5dbd5123",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\gradient_descent.py:111: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "optimizer = keras.optimizers.SGD(lr = 0.005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "41f0f3ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer = optimizer, loss = \"BinaryCrossentropy\", metrics = [\"AUC\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29dbc8c9",
   "metadata": {},
   "source": [
    "### Training model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cee7d498",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.fit([train_x, train_hard[:, 0]], np.array(Y_train), batch_size = 32, epochs = 4, verbose = 1, validation_data = [[val_x, val_hard[:, 0]], np.array(Y_val)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba486e6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving to model file\n",
    "\n",
    "model.save_weights(\"auc74ckpt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1048504e",
   "metadata": {},
   "source": [
    "### testing the obtained model on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "55fa0de4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_block():\n",
    "    return keras.Sequential([\n",
    "        ZeroPadding2D(padding = (1, 1)),\n",
    "        Conv2D(64, 3, padding = \"valid\", activation = \"ReLU\"),\n",
    "        BatchNormalization(axis = -1),\n",
    "        MaxPool2D(pool_size = 2)\n",
    "    ])\n",
    "\n",
    "inputs = Input(shape = (128, 128, 3))\n",
    "hard_feats = Input(shape = (1, ))\n",
    "x = conv_block()(inputs)\n",
    "x = conv_block()(x)\n",
    "x = conv_block()(x)\n",
    "x = conv_block()(x)\n",
    "x = Flatten()(x)\n",
    "x = Dense(32)(x)\n",
    "x = concatenate([x, hard_feats])\n",
    "x = BatchNormalization(axis = -1)(x)\n",
    "x = Dense(2, activation = \"sigmoid\")(x)\n",
    "\n",
    "test_model = Model(inputs = [inputs, hard_feats], outputs = [x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "c5e47ea2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.checkpoint.checkpoint.CheckpointLoadStatus at 0x1fb123cb8e0>"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_model.load_weights(\"auc74ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "d1652235",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_model.compile(optimizer = optimizer, loss = \"BinaryCrossentropy\", metrics = [\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eff3e637",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_model.evaluate([test_x, test_hard[:, 0]], np.array(Y_test), batch_size = 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2538df42",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
